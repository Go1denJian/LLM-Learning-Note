import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model

# ===== 1. åŠ è½½æ¨¡å‹ =====
model_name = r"D:\AI-explorer\models\tiny-gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))

# ===== 2. é…ç½® LoRA =====
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ===== 3. è¯»å–æ•°æ® =====
with open("data.json", "r") as f:
    raw_data = json.load(f)

def preprocess(example):
    prompt = f"{example['instruction']}\n{example['input']}"
    full_text = prompt + " " + example["output"]

    tokenized = tokenizer(
        full_text,
        truncation=True,
        padding="max_length",
        max_length=64
    )

    # ğŸ”‘ å…³é”®ï¼šlabels = input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()

    return tokenized


dataset = [preprocess(x) for x in raw_data]

# ===== 4. è®­ç»ƒå‚æ•° =====
training_args = TrainingArguments(
    output_dir="./lora_out",
    per_device_train_batch_size=2,
    num_train_epochs=10,
    logging_steps=1,
    save_strategy="no",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

# ===== 5. å¼€å§‹è®­ç»ƒ =====
trainer.train()

# ===== 6. ä¿å­˜ LoRA æƒé‡ =====
model.save_pretrained("lora_adapter")
